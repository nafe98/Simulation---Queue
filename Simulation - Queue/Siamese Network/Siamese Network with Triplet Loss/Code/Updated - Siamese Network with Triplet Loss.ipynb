{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veUkNpOgzlJl",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Siamese Network with Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnd9gBVxzlJn"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GK2uJHboz-Bv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLGUMKYpzlJp"
   },
   "source": [
    "## Understanding the Approach\n",
    "\n",
    "This appraoch is taken from the popular [FaceNet](https://arxiv.org/abs/1503.03832) paper.\n",
    "\n",
    "We have a CNN model called `EmbeddingModel`:\n",
    "\n",
    "![CNN](assets/CNN.png)\n",
    "\n",
    "We use three images for each training example:\n",
    "1. `person1_image1.jpg` (Anchor Example, represented below in green)\n",
    "2. `person1_image2.jpg` (Positive Example, in blue)\n",
    "3. `person2_image1.jpg` (Negative Example, in red).\n",
    "\n",
    "![Embeddings](assets/embeddings.png)\n",
    "\n",
    "\n",
    "## Siamese Network\n",
    "\n",
    "All the three images of an example pass through the model, and we get the three Embeddings: One for the Anchor Example, one for the Positive Example, and one for the Negative Example.\n",
    "\n",
    "![Siamese Network](assets/siamese.png)\n",
    "\n",
    "The three instances of the `EmbeddingModel` shown above are not different instances. It's the same, shared model instance - i.e. the parameters are shared, and are updated for all the three paths simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pi6cABUzlJp"
   },
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFUusF1Bztbr",
    "outputId": "e0745210-b8b4-4518-f707-97e8f7c4a4f8"
   },
   "outputs": [],
   "source": [
    "x_train = np.load('../Dataset/X_train.npy')\n",
    "y_train = np.load('../Dataset/y_train.npy')\n",
    "x_test = np.load('../Dataset/X_test.npy')\n",
    "y_test = np.load('../Dataset/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whrqCFTSzlJp",
    "outputId": "1a491803-c2a1-41eb-ca03-c40d5bf0aa87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(621158, 5)\n",
      "(621158,)\n",
      "(155290, 5)\n",
      "(155290,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPjhFU4JzlJq"
   },
   "source": [
    "# A Batch of Triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_batch function constructs a batch of triplets for training a triplet network. It initializes arrays to store anchors, positives, and negatives. For each entry in the batch, it randomly selects an anchor and its label from the training data, then selects a positive example with the same label and a negative example with a different label. These examples are assigned to the corresponding arrays. The function returns a list of the three arrays: anchors, positives, and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d7iKzOqUzlJq"
   },
   "outputs": [],
   "source": [
    "def create_batch(batch_size, x_train, y_train):\n",
    "    anchors = np.zeros((batch_size, 5))\n",
    "    positives = np.zeros((batch_size, 5))\n",
    "    negatives = np.zeros((batch_size, 5))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        index = random.randint(0, len(x_train) - 1)\n",
    "        anc = x_train[index]\n",
    "        y = y_train[index]\n",
    "\n",
    "        indices_for_pos = np.squeeze(np.where(y_train == y))\n",
    "        indices_for_neg = np.squeeze(np.where(y_train != y))\n",
    "\n",
    "        pos = x_train[indices_for_pos[random.randint(0, len(indices_for_pos) - 1)]]\n",
    "        neg = x_train[indices_for_neg[random.randint(0, len(indices_for_neg) - 1)]]\n",
    "\n",
    "        anchors[i] = anc\n",
    "        positives[i] = pos\n",
    "        negatives[i] = neg\n",
    "\n",
    "    return [anchors, positives, negatives]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37o8PEjJzlJq"
   },
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding model transforms input data into a 64-dimensional embedding space using a Keras Sequential model. It consists of an input dense layer with 64 units and ReLU activation for initial processing, followed by a dense embedding layer with 64 units and sigmoid activation to produce the final embeddings. This architecture effectively maps input features to a lower-dimensional space, supporting the triplet loss training by distinguishing between similar and dissimilar examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8J-cNOxzlJq",
    "outputId": "48b5fad0-e811-4fb0-d824-84df7ece8487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 64)                384       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,544\n",
      "Trainable params: 4,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 64\n",
    "\n",
    "embedding_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(5,)),\n",
    "    tf.keras.layers.Dense(emb_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFp-OTvjzlJq"
   },
   "source": [
    "# Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Siamese network processes triplets of inputs (anchor, positive, and negative) by applying a shared embedding model to each, generating 64-dimensional embeddings. It has three input layers, applies the embedding model to each input, and then concatenates the resulting embeddings. This concatenated output is used for training with the triplet loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tL9tCAsXzlJq",
    "outputId": "7a92413f-c170-469b-cbac-f4ea55d4a101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 64)           4544        ['input_4[0][0]',                \n",
      "                                                                  'input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 192)          0           ['sequential_5[0][0]',           \n",
      "                                                                  'sequential_5[1][0]',           \n",
      "                                                                  'sequential_5[2][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,544\n",
      "Trainable params: 4,544\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "in_anc = tf.keras.layers.Input(shape=(5,))\n",
    "in_pos = tf.keras.layers.Input(shape=(5,))\n",
    "in_neg = tf.keras.layers.Input(shape=(5,))\n",
    "\n",
    "# Apply the embedding model to the inputs\n",
    "em_anc = embedding_model(in_anc)\n",
    "em_pos = embedding_model(in_pos)\n",
    "em_neg = embedding_model(in_neg)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "out = tf.keras.layers.Concatenate(axis=1)([em_anc, em_pos, em_neg])\n",
    "\n",
    "# Define the model\n",
    "net = tf.keras.Model(inputs=[in_anc, in_pos, in_neg], outputs=out)\n",
    "\n",
    "# Print the model summary\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqOfsyBUzlJq"
   },
   "source": [
    "# Triplet Loss\n",
    "\n",
    "A loss function that tries to pull the Embeddings of Anchor and Positive Examples closer, and tries to push the Embeddings of Anchor and Negative Examples away from each other.\n",
    "\n",
    "Root mean square difference between Anchor and Positive examples in a batch of N images is:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p = \\sqrt{\\frac{\\sum_{i=0}^{N-1}(f(a_i) - f(p_i))^2}{N}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Root mean square difference between Anchor and Negative examples in a batch of N images is:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_n = \\sqrt{\\frac{\\sum_{i=0}^{N-1}(f(a_i) - f(n_i))^2}{N}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "For each example, we want:\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p \\leq d_n\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Therefore,\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p - d_n \\leq 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This condition is quite easily satisfied during the training.\n",
    "\n",
    "We will make it non-trivial by adding a margin (alpha):\n",
    "$\n",
    "\\begin{equation}\n",
    "d_p - d_n + \\alpha \\leq 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Given the condition above, the Triplet Loss L is defined as:\n",
    "$\n",
    "\\begin{equation}\n",
    "L = max(d_p - d_n + \\alpha, 0)\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triplet_loss function defines a custom loss for triplet networks by computing the Euclidean distances between anchor-positive and anchor-negative pairs from the model's predictions. It extracts the anchor, positive, and negative embeddings, calculates the squared distances between the anchor and the positive (dp) and between the anchor and the negative (dn), and then computes the triplet loss as max(dp - dn + alpha, 0.0). This loss ensures that the distance between anchor-positive pairs is smaller than that between anchor-negative pairs by at least a margin alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dzeDbEa8zlJr"
   },
   "outputs": [],
   "source": [
    "def triplet_loss(alpha, emb_dim):\n",
    "    def loss(y_true, y_pred):\n",
    "        anc, pos, neg = y_pred[:, :emb_dim], y_pred[:, emb_dim:2*emb_dim], y_pred[:, 2*emb_dim:]\n",
    "\n",
    "        # Calculate the Euclidean distance between anchor-positive and anchor-negative\n",
    "        dp = tf.reduce_mean(tf.square(anc - pos), axis=1)\n",
    "        dn = tf.reduce_mean(tf.square(anc - neg), axis=1)\n",
    "\n",
    "        # Compute the triplet loss\n",
    "        triplet_loss = tf.maximum(dp - dn + alpha, 0.0)\n",
    "        return triplet_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTgqFzeMzlJr"
   },
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_generator function creates an infinite loop that yields batches of triplets for training a triplet network. It calls the create_batch function to generate triplets of anchor, positive, and negative examples, and produces a corresponding label array filled with zeros of shape (batch_size, 3 * emb_dim) to match the model's output dimensions. The generator yields these batches and labels continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6FXEB5POzlJr"
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, emb_dim):\n",
    "    while True:\n",
    "        x = create_batch(batch_size, x_train, y_train)\n",
    "        y = np.zeros((batch_size, 3 * emb_dim))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxs9Zh75zlJr"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained by first configuring parameters: a batch size of 1024, 5 epochs, and steps per epoch calculated based on the dataset size. The model is compiled with a triplet loss function, using an Adam optimizer. A PCAPlotter instance is created for visualizing the embeddings. The training is executed with the fit method, utilizing a data generator for batches of triplets and including the PCAPlotter as a callback to monitor embedding changes throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VzFK84cCzlJr"
   },
   "outputs": [],
   "source": [
    "# Define batch_size, epochs, and steps_per_epoch\n",
    "batch_size = 1024\n",
    "epochs = 200\n",
    "steps_per_epoch = int(len(x_train) / batch_size)\n",
    "\n",
    "# Compile the model\n",
    "net.compile(loss=triplet_loss(alpha=0.2, emb_dim=emb_dim), optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add EarlyStopping and ModelCheckpoint callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and record the history\n",
    "history = net.fit(\n",
    "    data_generator(batch_size, emb_dim),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=True,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.title('Triplet Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, davies_bouldin_score, roc_auc_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for training and test sets\n",
    "train_embeddings = embedding_model.predict(x_train)\n",
    "test_embeddings = embedding_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_embeddings, y_train)\n",
    "y_pred = knn.predict(test_embeddings)\n",
    "knn_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('k-NN Accuracy:', knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "sil_score = silhouette_score(test_embeddings, y_test)\n",
    "print('Silhouette Score:', sil_score)\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "db_index = davies_bouldin_score(test_embeddings, y_test)\n",
    "print('Davies-Bouldin Index:', db_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Visualization\n",
    "tsne = TSNE(n_components=2)\n",
    "test_embeddings_2d = tsne.fit_transform(test_embeddings)\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in np.unique(y_test):\n",
    "    mask = y_test == i\n",
    "    plt.scatter(test_embeddings_2d[mask, 0], test_embeddings_2d[mask, 1], \n",
    "                label=f'Class {i}', s=5)\n",
    "plt.title('t-SNE Visualization of Test Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Visualization\n",
    "pca = PCA(n_components=2)\n",
    "test_embeddings_2d = pca.fit_transform(test_embeddings)\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in np.unique(y_test):\n",
    "    mask = y_test == i\n",
    "    plt.scatter(test_embeddings_2d[mask, 0], test_embeddings_2d[mask, 1], \n",
    "                label=f'Class {i}', s=5)\n",
    "plt.title('PCA Visualization of Test Embeddings')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
