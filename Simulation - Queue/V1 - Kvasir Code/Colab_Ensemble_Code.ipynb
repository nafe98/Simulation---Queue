{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCOKdGieyPQq"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wB9iBAX09ijp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9zTLJDXyPQr"
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsUEr7tL9lMK",
    "outputId": "ad1b5d38-b963-40e1-b449-16d724200653"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set the correct dataset path\n",
    "dataset_path = 'test_train_valid'\n",
    "\n",
    "# Verify the dataset path\n",
    "import os\n",
    "print(os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pULOgGYV9lOe"
   },
   "outputs": [],
   "source": [
    "img_size = (244, 244)\n",
    "batch_size = 32\n",
    "train_dir= 'test_train_valid/train'\n",
    "val_dir= 'test_train_valid/validation'\n",
    "test_dir= 'test_train_valid/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIseRSuJyPQs"
   },
   "source": [
    "# Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW84ym-U9lQ6",
    "outputId": "8925c5dc-418c-4468-85e5-2a4d1f2b152a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Training set\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  seed=123,\n",
    "  image_size=img_size,\n",
    "  batch_size=batch_size,\n",
    "  #color_mode='grayscale',\n",
    "  label_mode = 'categorical')\n",
    "\n",
    "# Validation set\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  val_dir,\n",
    "  seed=123,\n",
    "  image_size=img_size,\n",
    "  batch_size=batch_size,\n",
    "  #color_mode='grayscale',\n",
    "  label_mode = 'categorical')\n",
    "\n",
    "# Test set\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  seed=123,\n",
    "  image_size=img_size,\n",
    "  batch_size=batch_size,\n",
    "  #color_mode='grayscale',\n",
    "  label_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VExv4Qk9lTa"
   },
   "outputs": [],
   "source": [
    "def norm_data (ds):\n",
    "  prepr_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "  norm_ds = ds.map(lambda x, y: (prepr_layer(x), y))\n",
    "\n",
    "  return norm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJeooIfl9lVy"
   },
   "outputs": [],
   "source": [
    "def augment_data1 (ds, crop_size):\n",
    "\n",
    "  layer1 = tf.keras.layers.experimental.preprocessing.CenterCrop(crop_size, crop_size)\n",
    "  aug_ds = ds.map(lambda x, y: (layer1(x), y))\n",
    "\n",
    "  layer2 = tf.keras.layers.experimental.preprocessing.RandomRotation(factor=(-0.05, 0.05) )\n",
    "  aug_ds = aug_ds.map(lambda x, y: (layer2(x), y))\n",
    "\n",
    "  layer3 = tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=(-0.1, -0.01), width_factor=(-0.1,  -0.01))\n",
    "  aug_ds = aug_ds.map(lambda x, y: (layer3(x), y))\n",
    "\n",
    "  return aug_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQSMOtlA9lX6"
   },
   "outputs": [],
   "source": [
    "img_height = 244\n",
    "train_ds_aug = augment_data1 (train_ds, img_height-20)\n",
    "train_ds_norm = norm_data (train_ds_aug)\n",
    "val_ds_aug = augment_data1 (val_ds, img_height-20)\n",
    "val_ds_norm = norm_data (val_ds_aug)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds_norm = train_ds_norm.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_norm = val_ds_norm.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds_aug = train_ds_aug.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds_aug = val_ds_aug.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds_aug = augment_data1 (test_ds, img_height-20)\n",
    "test_ds_norm = norm_data (test_ds_aug)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "test_ds_norm = test_ds_norm.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf3BJKPWyPQt"
   },
   "source": [
    "# Ensemble Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mr_zd8iqLu9F",
    "outputId": "dd7f4765-524d-4430-bb9d-8d3b3be67287"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "checkpoint_path = \"Ensemble_Checkpoint/Ensemble_Checkpoint.h5\"\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Load pre-trained models\n",
    "model_CNN = load_model('/content/drive/My Drive/Kvasir Code/CNN.h5')\n",
    "#model_dnsnt201 = load_model('/content/drive/My Drive/Kvasir Code/dnsnt201.h5')\n",
    "#model_rsnet152 = load_model('/content/drive/My Drive/Kvasir Code/rsnt152.h5')\n",
    "model_rsnt101 = load_model('/content/drive/My Drive/Kvasir Code/rsnt101.h5')\n",
    "\n",
    "# Remove top layers to get feature extractors\n",
    "model_CNN_feature = tf.keras.Model(inputs=model_CNN.input, outputs=model_CNN.layers[-2].output)\n",
    "#model_dnsnt201_feature = tf.keras.Model(inputs=model_dnsnt201.input, outputs=model_dnsnt201.layers[-2].output)\n",
    "#model_rsnet152_feature = tf.keras.Model(inputs=model_rsnet152.input, outputs=model_rsnet152.layers[-2].output)\n",
    "model_rsnt101_feature = tf.keras.Model(inputs=model_rsnt101.input, outputs=model_rsnt101.layers[-2].output)\n",
    "\n",
    "# Input layer\n",
    "num_classes = 8\n",
    "input_shape = (224, 224, 3)\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Extract features\n",
    "CNN_features = model_CNN_feature(inputs)\n",
    "#dnsnt201_features = model_dnsnt201_feature(inputs)\n",
    "#rsnet152_features = model_rsnet152_feature(inputs)\n",
    "rsnt101_features = model_rsnt101_feature(inputs)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = Concatenate()([CNN_features, rsnt101_features]) #, dnsnt201_features, rsnet152_features])\n",
    "\n",
    "# Add new dense layers on top\n",
    "x = Dense(512, activation='relu')(combined_features)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the new ensemble model\n",
    "ensemble_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile and train the ensemble model\n",
    "opt1 = optimizers.Adam(learning_rate=2e-5)\n",
    "ensemble_model.compile(optimizer=opt1, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "history_ensemble = ensemble_model.fit(\n",
    "    train_ds_norm,\n",
    "    epochs=1,\n",
    "    validation_data=val_ds_norm,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63ThZ7IbLu_1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aIkVqrkMTp5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLDYNkk3MTsM"
   },
   "outputs": [],
   "source": [
    "# Plotting the Loss and Accuracy Curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_ensemble.history['loss'], label='train_loss')\n",
    "plt.plot(history_ensemble.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_ensemble.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history_ensemble.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = ensemble_model.evaluate(val_ds_norm)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_prob = ensemble_model.predict(val_ds_norm)\n",
    "y_pred = tf.argmax(y_pred_prob, axis=1).numpy()\n",
    "\n",
    "# Get true labels\n",
    "y_true = []\n",
    "for images, labels in val_ds_norm:\n",
    "    y_true.extend(tf.argmax(labels, axis=1).numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=val_ds_norm.class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=val_ds_norm.class_names, yticklabels=val_ds_norm.class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy, precision, recall, F1-score, AUC-ROC\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(tf.one_hot(y_true, depth=num_classes), y_pred_prob, average='weighted', multi_class='ovr')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GBC-eMjMTvF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTh08mIOMTx8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZSdwhqaMT0u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doY1mRDqMT3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DR6rWutMT6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHrvN8SLMT9W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zl0H1bzTMUAO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
